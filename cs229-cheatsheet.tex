\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\everymath{\displaystyle}



\pdfinfo{
  /Title (example.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Seamus)
  /Subject (Example)
  /Keywords (pdflatex, latex,pdftex,tex)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{CS229-Cheatsheet}} \\
\end{center}

\section{Supervised Learning}
\begin{itemize}
\item \textbf{Gradient Descent:} to minimize $J(\theta)$, we perform $\theta_j := \theta_j	 - \alpha \frac{\partial}{\partial \theta} J(\theta)$
\item $\nabla_A AB = B^T$ , $\nabla_{A^T} f(A) = (\nabla_A f(A))^T$, $\nabla_A tr ABA^TC = CAB + C^TAB^T$, $\nabla_A |A| = |A| (A^{-1})^T$

\item \textbf{Normal Equations and Least Squares} $J(\theta) = \frac{1}{2}\sum_{i = 1}^{m}(h_\theta(x^i)- y^i)^2 \rightarrow \nabla_\theta J(\theta)  = \nabla_\theta \frac{1}{2}(X\theta -y)^T(X\theta -y) =  X^TX\theta - X^Ty  = 0 \rightarrow X^TX\theta = X^Ty \rightarrow \theta = (X^TX)^{-1}X^Ty$. 
\item \textbf{Locally Weighted Regression} Fit $\theta$ to minimize $\sum_{i = 0}^{m	} (y^i-\theta^Tx^i)^2$ where $w^i = e^{-\frac{(x^i-x)^2}{2\tau^2}}$
\item \textbf{Logistic Regression:} $h_\theta(x) = g(\theta^T x) = \frac{1}{1+ e^{-\theta^Tx}}$, $g(z) = \frac{1}{1+e^{-z}}$, $g'(z) = \frac{d}{dz}\frac{1}{1+e^{-z}} = g(z)(1-g(z))$, $p(y |x; \theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$. $l(\theta) = log L(\theta) = \sum_{i = 1}^{m} y^i logh(x^i) + (1-y^i) log(1-h(x^i))$, $\frac{\partial}{\partial \theta_j} l(\theta) = (y- h_\theta(x))x_j$
\item \textbf{Perceptron Learning Algorithm} $\theta_j := \theta_j + \alpha (y^i - h_\theta(x^i))x_j^i$
\item \textbf{Newton's Method: } $\theta := \theta - \frac{f(\theta)}{f'(\theta)}$, we want the first derivative to be zero, then $\theta := \theta - \frac{l'(\theta)}{l''(\theta)}$, if $\theta$ is a vector then $\theta: = \theta - H^{-1}\nabla_\theta l(\theta)$ where $H_{ij} = \frac{\partial ^2 l(\theta}{\partial \theta_i \partial \theta_j}$
\item \textbf{Exponential Family}  $p(y; \eta)  =b(y)exp(\eta^TT(y) - a(\eta))$
\item \textbf{General Linear Model Assumptions:} 1. $y|x; \eta \sim ExponentialFamily(\eta)$. 2. Given $x$ our goal is to predict the expected value of $T(y)$ which is usually just $y$, so we would like our hypothesis to satisfy $h(x) = E(y|x)$. 3. The natural parameter $\eta$ and inputs $x$ are related linearly. $\eta = \theta^Tx$. 
\item \textbf{Canonical response function:} the distribution's mean as a function of the natural parameter $g(\eta) = E(T(y); \eta)$. 
\end{itemize}
\section*{Generative Learning Algorithm}
\subsection*{Gaussian Discriminant Analysis}
\begin{itemize}
\item $y \sim Bernoulli(\phi), x|y = 0 \sim N(\mu_0, \Sigma), x|y = 1 \sim N(\mu_1, \Sigma)$. 
\item $p(y) = \phi^y (1-\phi)^{1-y}$
\item $p(x|y=0) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))$
\item $p(x|y=1) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))$
\item $l(\phi, \mu_0, \mu_1, \Sigma) = log \prod_{i=1}^m p(x^i, y^i; \phi, \mu_0, \mu_1, \Sigma) = log \prod_{i=1}^m p(x^i| y^i; \phi, \mu_0, \mu_1, \Sigma) p(y^i, \phi)$. 
\item By maximizing $l$ with respect to the parameters, we find the maximum likelihood of the parameters to be:\\
$\phi= \frac{1}{m}1\{y^i = 1\}$\\
$\mu_0 = \frac{\sum_{i = 1}^{m}\{y^i = 0\}x^i}{\sum_{i = 1}^{m}\{y^i = 0\}}$\\
$\mu_1 = \frac{\sum_{i = 1}^{m}\{y^i = 1\}x^i}{\sum_{i = 1}^{m}\{y^i = 1\}}$\\
$\Sigma = \frac{1}{m}\sum_{i = 1}^{m} (x^i - \mu_{y^i})^T (x^i - \mu_{y^i})$
\end{itemize}
\subsection*{Naive Bayes}
\begin{itemize}
\item \textbf{Naive Assumption:} $p(x_1, x_2, \dots |y) = p(x_1|y)p(x_2|y) \dots. = \prod_{i = 1}^n p(x_i|y)$
\item \textbf{Laplace Smoothing}  $ \phi_j = \frac{\sum_{i = 1}^{m} 1\{z^i = j\}}{m} \rightarrow \frac{\sum_{i = 1}^{m} 1\{z^i = j +1\}}{m +k}$, where $k$ represent the number of possible outcomes for $z$. 
\item \textbf{Event Driven Text Classification:} \\
$\phi_{k|y = 1} = \frac{\sum_{i = 1}^{m}\sum_{j = 1}^{n_i} 1\{ x_j^i = k \wedge y^i = 1\}}{\sum_{i = 1}^{m}1\{y^i = 1\} n_i}$\\
$\phi_{k|y = 0} = \frac{\sum_{i = 1}^{m}\sum_{j = 1}^{n_i} 1\{ x_j^i = k \wedge y^i = 0\}}{\sum_{i = 1}^{m}1\{y^i = 0\} n_i}$\\
$\phi_y = \frac{\sum_{i = 1}^{m}1\{y^i = 1\}}{m}$
\end{itemize}
\section*{Support Vector Machines}
\begin{itemize}
\item \textbf{Classifier}: $h_{w, b}(x) = g(w^Tx+b) $ where $g(z) = 1$ if $z >0$ and $g(z) = -1$ otherwise. 
\item \textbf{Functional Margins:} $\hat{\gamma}^i = y^i (w^Tx +b)$, the smallest functional margin in the training set is called: $\hat{\gamma} = min_{i = 1, 2, \dots, m} \hat{\gamma}^i$
\item \textbf{Geometric Margins:} $\gamma^i = y^i ((\frac{w}{||w||})^T x^i + \frac{b}{||w||})$, the smallest geometric margin in a training set is :$ \gamma = min_{i = 1, \dots, m} \gamma^i$
\item \textbf{Optimal Margin Classifier}: $min_{\gamma, w, b} \frac{1}{2} ||w||^2$\\
s.t $y^i(w^Tx^i +b) \geq 1, i = 1, 2, \dots, m$
\item \textbf{Lagrangian} $ L(w, b, \alpha) = \frac{1}{2} ||w||^2 - \sum_{i = 1}^{m}\alpha_i (y^i (w^Tx^i +b)-1)$
\item \textbf{The dual problem } $max_\alpha W(\alpha) = \sum_{i = 1}^{m}\alpha_i - \frac{1}{2}\sum_{i, j = 1}^{m}y^i y^j \alpha_i \alpha_j (x^i)^Tx^j$\\
s.t $\sum_{i = 1}^{m}\alpha_i y^i = 0$\\
$\alpha_i \geq 0 for i = 1, \dots, m$
\item \textbf{Observations:} 1. Most of the $\alpha_i$s will be zero 2. $w^Tx +b = (\sum_{i = 1}^{m} \alpha_i y^i x^i )^T x +b $
\item \textbf{KKT Conditions:}
\begin{align*}
\frac{\partial }{\partial w_i} L(w^*, \alpha^*, \beta^*) &= 0, i = 1, \dots n\\
\frac{\partial }{\partial \beta_i} L(w^*, \alpha^*, \beta^*) &= 0, i = 1, \dots l\\
\alpha^*g_i(w^*) = 0, i &= 1, \dots, k\\
g_i(w^*) &\leq 0, i = 1, \dots, k\\
\alpha^* &\geq 0, i = 1, \dots, k
\end{align*}
\item \textbf{Mercer Theorem:} Let $K: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ be given, then for $K$ to be a valid kernel, it is necessary and sufficient that for any $\{x_1, x_2, \dots, x^m\}$, the corresponding kernel matrix is symmetric positive semi-definite. 
\item \textbf{Regularization (revised optimal margin classifier}\\$min_{\gamma, w, b} \frac{1}{2} ||w||^2 + C\sum_{i = 1}^{m}\xi_i $\\
s.t $y^i(w^Tx^i +b) \geq 1-\xi_i, i = 1, 2, \dots, m$ \\
$\xi_i \geq 0, i = 1, \dots, m$\\
\item \textbf{Dual of Regularization} \\
$max_\alpha W(\alpha) = \sum_{i = 1}^{m}\alpha_i - \frac{1}{2}\sum_{i, j = 1}^{m}y^i y^j \alpha_i \alpha_j (x^i)^Tx^j$\\
s.t $\sum_{i = 1}^{m}\alpha_i y^i = 0$\\
$C \geq \alpha_i \geq 0 for i = 1, \dots, m$
\end{itemize}
\section*{Learning Theory}
\begin{itemize}
\item \textbf{Union Bound:} Let $A_1, A_2, \dots, A_k$ be $k$ different events (that may not be independent). Then 
$$P(A_1 \cup A_2\dots A_k) \leq P(A_1) + P(A_2) + \dots P(A_k)$$
\item \textbf{Hoeffding inequality} Let $Z_1, Z_2, \dots Z_m$ be $m$ independent and identically distributed random variables drawn from Bernoulli($\phi$) distribution. Let $\hat{\phi}  = \frac{1}{m}\sum_{i = 1}^{m}Z_i$ be the mean of these random variables and let any $\gamma >0$ be fixed. Then 
$$P(|\phi - \hat{\phi}>\gamma ) \leq 2exp(-2\gamma m)$$
\item \textbf{Generational Error Bound:} $P(|\epsilon(h_i) - \hat{\epsilon}(h_i)| > \gamma) \leq 2exp(-2\gamma^2m)$
\item \textbf{Uniform Convergence:} \\
\begin{align*}
&P(\neg \exists h \in H. |\epsilon(h_i) - \hat{\epsilon}(h_i)| > \gamma) \\
&= P(\forall h \in H. |\epsilon(h_i) - \hat{\epsilon}(h_i)| \leq \gamma)\\
& \geq 1- 2k exp(-2\gamma^2m)
\end{align*}

\item \textbf{Solving $m, \gamma, \delta$} We just need to use the equation $\delta = 2k exp(-2\gamma^2m)$ to solve for one variable given the other two. 
\item  Let $|H| = k$, and let any $m, \delta,$ be fixed. Then with probability at least $ 1-\delta$, we have that 
$$
\epsilon(\hat{h})  \leq (\min_{h \in H} \epsilon(h)) + 2\sqrt{\frac{1}{2m} log(\frac{2k}{\delta})}
$$
\end{itemize}
% You can even have references
\rule{0.3\linewidth}{0.25pt}
\scriptsize
\bibliographystyle{abstract}
\bibliography{refFile}
\end{multicols}
\end{document}
